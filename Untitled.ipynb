{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score,r2_score, log_loss, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import probplot as qqplot\n",
    "from sklearn.svm.libsvm import predict_proba # need for the log_loss metric\n",
    "import warnings\n",
    "from scipy.stats import poisson\n",
    "import lizec\n",
    "import os\n",
    "import pickle \n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "pd.set_option('display.max_columns', 500)\n",
    "plt.style.use('classic')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "print(plt.rcParams.get('figure.figsize'))\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "#%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import avg, count, col, abs, max as spark_max, min as spark_min\n",
    "from pyspark.sql.functions import sum as spark_sum, unix_timestamp, udf\n",
    "import pyspark.sql.functions as psf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    conf = SparkConf().setMaster(\"local\").setAppName(\"My App2\").setSystemProperty('spark.executor.memory', '50mb')\n",
    "    sc = SparkContext(conf = conf)\n",
    "    sqlContext = SQLContext(sc)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark: cluster computing platform. Spark-core is the heart: a computing engine for\n",
    "# scheduling, distributing etc\n",
    "\n",
    "# jupyter already loads an SparkContext instance sc, like in pyspark shell\n",
    "\n",
    "\n",
    "# driver program: this notebook, shell or standaline app. Driver programs define RDDs\n",
    "# and actions on them. Driver programms access spark (spark core) through a SC object\n",
    "# Driver programs manages nodes/executers and deploy tasks to them.,\n",
    "\n",
    "# Create RDD from file. RDDs are immutable collections of objects. They are split into partitions\n",
    "# which may be computed on different nodes of the cluster\n",
    "#conf = SparkConf().setMaster(\"local\").setAppName(\"My App2\")\n",
    "#sc = SparkContext(conf = conf)\n",
    "\n",
    "\n",
    "lines = sc.textFile(\"README.md\")\n",
    "\n",
    "lines.first()\n",
    "\n",
    "# Create RDD from a collection of objects (e.g., a list or a set)\n",
    "values = sc.parallelize((1,2,3,4))\n",
    "\n",
    "def add_one(x):\n",
    "    return x+1\n",
    "\n",
    "# Transformations: operations on RDDs that return a new RDD. They are computed lazily:\n",
    "# only when you call them to action. At first, transformations return pointers to new RDDs.\n",
    "b = values.filter(lambda x: x <2)\n",
    "c = values.map(add_one)\n",
    "# Actions: Compute actual output to the driver program or to storage system\n",
    "print(b.collect(), c.collect())\n",
    "\n",
    "d = values.reduce(lambda x, y: x+y)\n",
    "d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---+---+---+\n",
      "| _1|   _2| _3| _4| _5| _6|\n",
      "+---+-----+---+---+---+---+\n",
      "|Eve|Alice|  1|  2|  3|  4|\n",
      "+---+-----+---+---+---+---+\n",
      "\n",
      "+---+---+---+---+---+---+\n",
      "| _1| _2| _3| _4| _5| _6|\n",
      "+---+---+---+---+---+---+\n",
      "|Eve|Bob|  2|  2|  2|  2|\n",
      "+---+---+---+---+---+---+\n",
      "\n",
      "+---+-----+---+---+---+---+\n",
      "| _1|   _2| _3| _4| _5| _6|\n",
      "+---+-----+---+---+---+---+\n",
      "|Eve|Alice|  1|  2|  3|  4|\n",
      "|Eve|  Bob|  2|  2|  2|  2|\n",
      "+---+-----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('Eve','Alice', 1,2,3,4)]\n",
    "rdd = sc.parallelize(l)\n",
    "sqlContext.createDataFrame(rdd)\n",
    "#DataFrame[_1: string, _2: bigint]\n",
    "df = sqlContext.createDataFrame(rdd)\n",
    "df.show()\n",
    "#dataframe = df.agg(avg('_3'))\n",
    "\n",
    "\n",
    "\n",
    "k = [('Eve', 'Bob',2,2,2,2)]\n",
    "rdd = sc.parallelize(k)\n",
    "sqlContext.createDataFrame(rdd)\n",
    "#DataFrame[_1: string, _2: bigint]\n",
    "df2 = sqlContext.createDataFrame(rdd)\n",
    "df2.show()\n",
    "\n",
    "\n",
    "df3 = df.unionAll(df2)\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single value rdd feature extraction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "#spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "\n",
    "\n",
    "#       Declarations\n",
    "# __________________\n",
    "def extr_ft_1 (proc_data, limit=100):\n",
    "    \n",
    "    proc_data = proc_data.filter(proc_data.speed > limit).agg(count(proc_data.speed))\n",
    "    \n",
    "    proc_data = proc_data.select(col('count(speed)').alias('speed_feature'))\n",
    "    \n",
    "    proc_data.show()\n",
    "    \n",
    "    return proc_data\n",
    "\n",
    "\n",
    "def extr_ft_0 (proc_data):\n",
    "    \n",
    "    max_t = proc_data.agg(spark_max(proc_data.timestamp))\n",
    "    \n",
    "    min_t = proc_data.agg(spark_min(proc_data.timestamp))\n",
    "    \n",
    "    max_t = max_t.select(col('max(timestamp)').alias('max'))\n",
    "    \n",
    "    min_t = min_t.select(col('min(timestamp)').alias('min'))\n",
    "\n",
    "    X = max_t.crossJoin(min_t)\n",
    "    \n",
    "    X = X.withColumn('time_feature', X.max+X.min)\n",
    "\n",
    "    X = X.drop(X.min).drop(X.max)\n",
    "    \n",
    "    X.show()\n",
    "    \n",
    "    return (X)\n",
    "\n",
    "\n",
    "\n",
    "def get_proc_features(proc, data, *features):\n",
    "    \n",
    "    proc_data = data.filter( data.customid == proc)\n",
    "    \n",
    "    features_for_proc = [feature_value(proc_data) for feature_value in features]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for number, feature in enumerate(features_for_proc):\n",
    "\n",
    "        if number == 0:\n",
    "        \n",
    "            l = [(proc,'dummy')]\n",
    "\n",
    "            rdd = sc.parallelize(l)\n",
    "\n",
    "            df = sqlContext.createDataFrame(rdd,['customid','dummy']) \n",
    "\n",
    "            df = df.drop(df.dummy)\n",
    "            \n",
    "            df.show()\n",
    "        \n",
    "            features_for_proc_rdd = feature\n",
    "            \n",
    "            features_for_proc_rdd = features_for_proc_rdd.crossJoin(df)\n",
    "        \n",
    "            continue\n",
    "         \n",
    "        features_for_proc_rdd = features_for_proc_rdd.crossJoin(feature)\n",
    "        \n",
    "        features_for_proc_rdd.show()\n",
    "        \n",
    "    return features_for_proc_rdd\n",
    "\n",
    "#       Main\n",
    "# __________________\n",
    "customer_list_1 = ['CM1', 'CX2']\n",
    "\n",
    "extr_feature_funcs = (extr_ft_0, extr_ft_1)\n",
    "\n",
    "l = [('CM1','aa1', 100,0.1),('CM1','aa1', 110,0.2),\\\n",
    "     ('CM1','aa1', 110,0.9),('CM1','aa1', 100,1.5),\\\n",
    "     ('CM1','xx2', 100,0.1),('CM1','xx2', 110,0.2),\\\n",
    "     ('CM1','xx2', 210,0.9),('CM1','xx2', 100,1.5),\\\n",
    "     ('CX2','bb9', 100,0.1),('CX2','bb9', 100,0.2),\\\n",
    "    ('CX2','bb9', 110,6.0),('CX2','bb9', 100,0.18)]\n",
    "\n",
    "rdd = sc.parallelize(l)\n",
    "\n",
    "df = sqlContext.createDataFrame(rdd,['customid','procid','speed','timestamp'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(df.schema)\n",
    "\n",
    "    \n",
    "for number, proc in  enumerate(customer_list_1):\n",
    "\n",
    "    if number == 0:\n",
    "        \n",
    "        #results = get_trip_features(trip, df, extr_ft_0, extr_ft_1)\n",
    "        results = get_proc_features(proc, df, *extr_feature_funcs)\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    results = results.unionAll(get_proc_features(proc, df, *extr_feature_funcs))\n",
    "    \n",
    "results.show()\n",
    "\n",
    "#results = results.toPandas()\n",
    "\n",
    "#results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|customid|max(timestamp)|\n",
      "+--------+--------------+\n",
      "|     CM1|           1.5|\n",
      "|     CX2|           6.0|\n",
      "+--------+--------------+\n",
      "\n",
      "+--------+------+-----+---------+--------------+\n",
      "|customid|procid|speed|timestamp|max(timestamp)|\n",
      "+--------+------+-----+---------+--------------+\n",
      "|     CM1|   aa1|  100|      0.1|           1.5|\n",
      "|     CM1|   aa1|  110|      0.2|           1.5|\n",
      "|     CM1|   aa1|  110|      0.9|           1.5|\n",
      "|     CM1|   aa1|  100|      1.5|           1.5|\n",
      "|     CM1|   xx2|  100|      0.1|           1.5|\n",
      "|     CM1|   xx2|  110|      0.2|           1.5|\n",
      "|     CM1|   xx2|  210|      0.9|           1.5|\n",
      "|     CM1|   xx2|  100|      1.5|           1.5|\n",
      "|     CX2|   bb9|  100|      0.1|           6.0|\n",
      "|     CX2|   bb9|  100|      0.2|           6.0|\n",
      "|     CX2|   bb9|  110|      6.0|           6.0|\n",
      "|     CX2|   bb9|  100|     0.18|           6.0|\n",
      "+--------+------+-----+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by customer and join with the base data\n",
    "data = df\n",
    "\n",
    "dataframe = data.groupBy('customid').agg(spark_max(data.timestamp))\n",
    "\n",
    "dataframe.show()\n",
    "\n",
    "rdd_join = df.join(dataframe, on='customid')\n",
    "\n",
    "rdd_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|procid|max(timestamp)|\n",
      "+------+--------------+\n",
      "|   bb9|           6.0|\n",
      "|   aa1|           1.5|\n",
      "|   xx2|           1.5|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same works for grouping on processid\n",
    "dummy = df.groupBy('procid').agg(spark_max(df.timestamp))\n",
    "\n",
    "dummy.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction with window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(customid,StringType,true),StructField(procid,StringType,true),StructField(speed,DoubleType,true),StructField(timestamp,TimestampType,true)))\n",
      "+--------+------+-----+-------------------+\n",
      "|customid|procid|speed|          timestamp|\n",
      "+--------+------+-----+-------------------+\n",
      "|     CM1|   aa1|  0.0|2017-05-30 20:00:00|\n",
      "|     CM1|   aa1|  5.0|2017-05-30 20:01:00|\n",
      "|     CM1|   aa1| 10.0|2017-05-30 20:02:00|\n",
      "|     CM1|   aa1| 15.0|2017-05-30 20:02:30|\n",
      "|     CM1|   bb1|  0.0|2017-05-30 19:00:00|\n",
      "|     CM1|   bb1|  3.0|2017-05-30 19:01:00|\n",
      "|     CM1|   bb1|  6.0|2017-05-30 19:02:00|\n",
      "|     CM1|   bb1|  9.0|2017-05-30 19:09:00|\n",
      "|     CM2|   cc2|  2.0|2017-05-30 23:00:00|\n",
      "|     CM2|   cc2|  4.0|2017-05-30 23:01:00|\n",
      "|     CM2|   cc2|  8.0|2017-05-30 23:02:00|\n",
      "|     CM2|   cc2|  2.0|2017-05-30 23:03:00|\n",
      "+--------+------+-----+-------------------+\n",
      "\n",
      "+------+-------------+-------------------+-------------------+--------------------+\n",
      "|procid|speed_feature|          Starttime|            Endtime|        TripDuration|\n",
      "+------+-------------+-------------------+-------------------+--------------------+\n",
      "|   aa1|          3.0|2017-05-30 20:00:00|2017-05-30 20:02:30|0.041666666666666664|\n",
      "|   cc2|          1.0|2017-05-30 23:00:00|2017-05-30 23:03:00|                0.05|\n",
      "|   bb1|          2.0|2017-05-30 19:00:00|2017-05-30 19:09:00|                0.15|\n",
      "+------+-------------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "# Define a windowing column.\n",
    "# Pyspark.sql.functions.Column.over\n",
    "\n",
    "def s_to_h(x): return x/(60*60)\n",
    "my_func = udf(s_to_h, DoubleType())\n",
    "\n",
    "def extr_ft_1 (proc_data, w, limit=4):\n",
    "    return proc_data.withColumn(\n",
    "        \"speed_feature\", \n",
    "        psf.sum((proc_data.speed > limit).cast(\"float\")).over(w)\n",
    "    )\n",
    "\n",
    "\n",
    "def extr_ft_0(proc_data, w):\n",
    "    proc_data = proc_data.withColumn(\n",
    "        \"Starttime\", \n",
    "        psf.min(proc_data.timestamp).over(w)\n",
    "        )\n",
    "    \n",
    "    proc_data = proc_data.withColumn(\n",
    "        \"Endtime\", \n",
    "        psf.max(proc_data.timestamp).over(w)\n",
    "        )\n",
    "    time_diff = (unix_timestamp(proc_data.Endtime)-unix_timestamp(proc_data.Starttime))\n",
    "    proc_data = proc_data.withColumn(\n",
    "        \"TripDurationSeconds\", \n",
    "        time_diff\n",
    "        )\n",
    "    proc_data = proc_data.withColumn(\n",
    "        \"TripDuration\", \n",
    "        my_func('TripDurationSeconds')\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return proc_data\n",
    "\n",
    "\n",
    "w = Window.partitionBy(\"procid\")\n",
    "\n",
    "\n",
    "l = [('CM1','aa1', 00.0,datetime.datetime(2017, 5, 30, 20,0,0)),\\\n",
    "     ('CM1','aa1', 5.0,datetime.datetime(2017, 5, 30, 20,1,0)),\\\n",
    "     ('CM1','aa1', 10.0,datetime.datetime(2017, 5, 30, 20,2,0)),\\\n",
    "     ('CM1','aa1', 15.0,datetime.datetime(2017, 5, 30, 20,2,30)),\\\n",
    "     \n",
    "     ('CM1','bb1', 00.0,datetime.datetime(2017, 5, 30, 19,0,0)),\\\n",
    "     ('CM1','bb1', 3.0,datetime.datetime(2017, 5, 30, 19,1,0)),\\\n",
    "     ('CM1','bb1', 6.0,datetime.datetime(2017, 5, 30, 19,2,0)),\\\n",
    "     ('CM1','bb1', 9.0,datetime.datetime(2017, 5, 30, 19,9,0)),\\\n",
    "     \n",
    "     ('CM2','cc2', 2.0,datetime.datetime(2017, 5, 30, 23,0,0)),\\\n",
    "     ('CM2','cc2', 4.0,datetime.datetime(2017, 5, 30, 23,1,0)),\\\n",
    "     ('CM2','cc2', 8.0,datetime.datetime(2017, 5, 30, 23,2,0)),\\\n",
    "     ('CM2','cc2', 2.0,datetime.datetime(2017, 5, 30, 23,3,0)),\\\n",
    "]\n",
    "\"\"\"\n",
    "l = [('CM1','aa1', 00.0,2015-01-02 22:59:58),('CM1','aa1', 10.0,0.2),\\\n",
    "     ('CM1','aa1', 10.0,2015-01-02 22:59:58),('CM1','aa1', 00.0,1.5),\\\n",
    "     ('CM1','xx2', 100.0,0.1),('CM1','xx2', 110.0,0.2),\\\n",
    "     ('CM1','xx2', 210.0,0.9),('CM1','xx2', 100.0,1.5),\\\n",
    "     ('CX2','bb9', 100.0,0.1),('CX2','bb9', 100.0,0.2),\\\n",
    "    ('CX2','bb9', 110.0,6.0),('CX2','bb9', 100.0,0.18)]\n",
    "\"\"\"\n",
    "schema = StructType([StructField('customid', StringType(), True),\n",
    "                     StructField('procid', StringType(), True),\n",
    "                     StructField('speed', DoubleType(), True),\n",
    "                     StructField('timestamp', TimestampType(), True)]\n",
    "                     )\n",
    "\n",
    "rdd = sc.parallelize(l)\n",
    "\n",
    "df = sqlContext.createDataFrame(rdd,schema)\n",
    "\n",
    "print(df.schema)\n",
    "\n",
    "df.show()\n",
    "df1 = extr_ft_1(df, w)\n",
    "df0 = extr_ft_0(df1, w)\n",
    "#df0.show()\n",
    "\n",
    "df0=df0[['procid', 'speed_feature', 'Starttime', 'Endtime', 'TripDuration' ]].distinct()\n",
    "df0.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append accel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+----+-------------------+----------+-----+\n",
      "|customid|procid|speed|wait|          timestamp|      u_ts|delay|\n",
      "+--------+------+-----+----+-------------------+----------+-----+\n",
      "|     CM1|   aa1|  3.0|null|2017-05-30 20:00:01|1496167201| null|\n",
      "|     CM1|   aa1| null| 0.1|2017-05-30 20:00:04|1496167204|    3|\n",
      "|     CM1|   aa1| null| 0.2|2017-05-30 20:00:08|1496167208|    4|\n",
      "|     CM1|   aa1| null| 0.3|2017-05-30 20:00:12|1496167212|    4|\n",
      "|     CM1|   aa1| null| 0.4|2017-05-30 20:00:30|1496167230|   18|\n",
      "|     CM1|   aa1| null| 0.0|2017-05-30 20:00:33|1496167233|    3|\n",
      "|     CM1|   aa1|  2.0|null|2017-05-30 20:00:37|1496167237|    4|\n",
      "|     CM1|   aa1| null| 0.1|2017-05-30 20:00:39|1496167239|    2|\n",
      "|     CM1|   aa1| null| 0.0|2017-05-30 20:00:39|1496167239|    0|\n",
      "|     CM1|   aa1| null| 0.2|2017-05-30 20:00:49|1496167249|   10|\n",
      "|     CM1|   aa1| null| 0.8|2017-05-30 20:00:55|1496167255|    6|\n",
      "|     CM1|   aa1|  4.0|null|2017-05-30 20:00:59|1496167259|    4|\n",
      "+--------+------+-----+----+-------------------+----------+-----+\n",
      "\n",
      "+--------+------+-----+----+-------------------+----------+-----+\n",
      "|customid|procid|speed|wait|          timestamp|      u_ts|delay|\n",
      "+--------+------+-----+----+-------------------+----------+-----+\n",
      "|     CM1|   aa1|  3.0|null|2017-05-30 20:00:01|1496167201| null|\n",
      "|     CM1|   aa1|  3.3| 0.1|2017-05-30 20:00:04|1496167204|    3|\n",
      "|     CM1|   aa1|  4.1| 0.2|2017-05-30 20:00:08|1496167208|    4|\n",
      "|     CM1|   aa1|  5.3| 0.3|2017-05-30 20:00:12|1496167212|    4|\n",
      "|     CM1|   aa1| 12.5| 0.4|2017-05-30 20:00:30|1496167230|   18|\n",
      "|     CM1|   aa1| 12.5| 0.0|2017-05-30 20:00:33|1496167233|    3|\n",
      "|     CM1|   aa1|  2.0|null|2017-05-30 20:00:37|1496167237|    4|\n",
      "|     CM1|   aa1|  2.2| 0.1|2017-05-30 20:00:39|1496167239|    2|\n",
      "|     CM1|   aa1|  2.2| 0.0|2017-05-30 20:00:39|1496167239|    0|\n",
      "|     CM1|   aa1|  4.2| 0.2|2017-05-30 20:00:49|1496167249|   10|\n",
      "|     CM1|   aa1|  9.0| 0.8|2017-05-30 20:00:55|1496167255|    6|\n",
      "|     CM1|   aa1|  4.0|null|2017-05-30 20:00:59|1496167259|    4|\n",
      "+--------+------+-----+----+-------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l = [('CM1','aa1',  3.0,  None, datetime.datetime(2017, 5, 30, 20,0,1)),\\\n",
    "     ('CM1','aa1',  None,    .1, datetime.datetime(2017, 5, 30, 20,0,4)),\\\n",
    "     ('CM1','aa1',  None,    .2, datetime.datetime(2017, 5, 30, 20,0,8)),\\\n",
    "     ('CM1','aa1',  None,    .3, datetime.datetime(2017, 5, 30, 20,0,12)),\\\n",
    "     ('CM1','aa1',  None,     .4, datetime.datetime(2017, 5, 30, 20,0,30)),\\\n",
    "     ('CM1','aa1',  None,    .0, datetime.datetime(2017, 5, 30, 20,0,33)),\\\n",
    "     ('CM1','aa1', 2.0,    None, datetime.datetime(2017, 5, 30, 20,0,37)),\\\n",
    "     ('CM1','aa1',  None,    .1, datetime.datetime(2017, 5, 30, 20,0,39)),\\\n",
    "     ('CM1','aa1',  None,     .0, datetime.datetime(2017, 5, 30, 20,0,39)),\\\n",
    "     ('CM1','aa1',  None,     .2, datetime.datetime(2017, 5, 30, 20,0,49)),\\\n",
    "     ('CM1','aa1',  None,    .8, datetime.datetime(2017, 5, 30, 20,0,55)),\\\n",
    "     ('CM1','aa1',  4.0,  None, datetime.datetime(2017, 5, 30, 20,0,59))\n",
    "        ]\n",
    "\n",
    "schema = StructType([StructField('customid', StringType(), True),\n",
    "                     StructField('procid', StringType(), True),\n",
    "                     StructField('speed', DoubleType(), True),\n",
    "                     StructField('wait', DoubleType(), True),\n",
    "                     StructField('timestamp', TimestampType(), True)]\n",
    "                     )\n",
    "\n",
    "rdd = sc.parallelize(l)\n",
    "\n",
    "df = sqlContext.createDataFrame(rdd,schema)\n",
    "\n",
    "df = df.withColumn('u_ts', unix_timestamp(df.timestamp))\n",
    "\n",
    "w = \\\n",
    "  Window.partitionBy(df['procid']).orderBy(df['timestamp'].asc())#.rangeBetween(-1, 0)\n",
    "\n",
    "df = df.withColumn('delay', (psf.lag(df.u_ts, 0).over(w))-(psf.lag(df.u_ts, 1).over(w)))\n",
    "    \n",
    "df.show()\n",
    "\n",
    "\n",
    "w = \\\n",
    "  Window.partitionBy(df['procid']).orderBy(df['timestamp'].asc())#.rangeBetween(-1, 0)\n",
    " \n",
    "\n",
    "for i in range(8):\n",
    "    \"\"\"\n",
    "    df = df.withColumn('windowresult1',\n",
    "                 (psf.lag(df.wait, 0).over(w)))\n",
    "\n",
    "    df = df.withColumn('windowresult2',\n",
    "                 psf.lag(df.speed, 1).over(w))\n",
    "    \"\"\"\n",
    "    df = df.withColumn('speed',\n",
    "                 psf.when(df.speed.isNull() == True, (psf.lag(df.wait, 0).over(w))*df.delay+psf.lag(df.speed, 1).over(w))\\\n",
    "                       .otherwise(df.speed))\n",
    "\n",
    "\n",
    "\n",
    "    #df = df.withColumn('speed',psf.coalesce(df.speed, df.result))\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List chunking (generator), multi-file pickling of pyspark dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy dataframe with box ids\n",
    "### Get a list with chunks of box-ids in string format for hiveContext\n",
    "### Dump the liste as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class BoxIdHandlers:\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.l = [('CM{}'.format(np.random.randint(100)), np.random.randint(2)) for i in range(100)]\n",
    "        self.df = pd.DataFrame(self.l, columns=['Id', 'crash'])\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def chunks(self, lizt, size):\n",
    "        \"\"\"Yield successive -sized chunks from list\"\"\"\n",
    "        for i in range(0, len(lizt), size):\n",
    "            \n",
    "            yield lizt[i:i + size]\n",
    "    \n",
    "    def getBoxChunksAsStrings(self, chunksize= 10, totalnumber= 100):\n",
    "        \n",
    "        boxes = self.df.loc[self.df.crash ==  0 ].Id.tolist() + self.df.loc[self.df.crash > 0 ].Id.tolist()\n",
    "        \n",
    "        random.shuffle(boxes)\n",
    "        \n",
    "        boxes = boxes[:totalnumber]\n",
    "            \n",
    "        return ['\"'+'\",\"'.join(chunk)+'\"' for chunk in self.chunks(boxes,chunksize)]\n",
    "        #return [chunk for chunk in chunks(boxes,chunksize)]\n",
    "    \n",
    "    \n",
    "    def pickleNdump(seld, boxlist, filename = 'boxstring'):\n",
    "        \n",
    "        script_dir = os.path.dirname(os.getcwd()) #<-- absolute dir the script is in\n",
    "        rel_path = \"boxlists/{}.pkl\".format(filename)\n",
    "        abs_file_path = os.path.join(script_dir, rel_path)\n",
    "        with open(abs_file_path, 'wb') as f:\n",
    "                pickle.dump(boxlist, f)\n",
    "    \n",
    "    \n",
    "\n",
    "BoxIdHandler = BoxIdHandlers()\n",
    "\n",
    "v = BoxIdHandler.getBoxChunksAsStrings(chunksize = 2, totalnumber = 11)\n",
    "\n",
    "BoxIdHandler.pickleNdump(v, filename = 'boxstringgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the pickled list an test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"CM62\",\"CM39\"\n",
      "/home/lizecallys/Dropbox/Schatztruhe 911/J's Kram/IT_Projekte/Python/boxlists/boxstring.pkl\n",
      "\"CM93\",\"CM97\"\n",
      "/home/lizecallys/Dropbox/Schatztruhe 911/J's Kram/IT_Projekte/Python/boxlists/boxstring.pkl\n",
      "\"CM46\",\"CM80\"\n",
      "/home/lizecallys/Dropbox/Schatztruhe 911/J's Kram/IT_Projekte/Python/boxlists/boxstring.pkl\n",
      "\"CM98\",\"CM36\"\n",
      "/home/lizecallys/Dropbox/Schatztruhe 911/J's Kram/IT_Projekte/Python/boxlists/boxstring.pkl\n",
      "\"CM36\",\"CM13\"\n",
      "/home/lizecallys/Dropbox/Schatztruhe 911/J's Kram/IT_Projekte/Python/boxlists/boxstring.pkl\n",
      "\"CM28\"\n",
      "/home/lizecallys/Dropbox/Schatztruhe 911/J's Kram/IT_Projekte/Python/boxlists/boxstring.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "script_dir = os.path.dirname(os.getcwd()) #<-- absolute dir the script is in\n",
    "rel_path = \"boxlists/boxstring.pkl\"\n",
    "abs_file_path = os.path.join(script_dir, rel_path)\n",
    "with open(abs_file_path, 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "for chunk in test: print(chunk), print(abs_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a file with dates. Return iteratively one date after another, if the date is not in the file. Write the date afterwards to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New day, writing 2016-10-02\n",
      "New day, writing 2016-10-03\n",
      "New day, writing 2016-10-04\n",
      "New day, writing 2016-10-05\n",
      "New day, writing 2016-10-06\n",
      "New day, writing 2016-10-07\n",
      "New day, writing 2016-10-08\n",
      "New day, writing 2016-10-09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_path():\n",
    "    script_dir = os.path.dirname(os.getcwd()) #<-- absolute dir the script is in\n",
    "    rel_path = \"processed_days.txt\"\n",
    "    abs_file_path = os.path.join(script_dir, rel_path)\n",
    "    return abs_file_path\n",
    "\n",
    "first = True\n",
    "for i in range(8):\n",
    "    start_day = dt.date(2016,10,1)\n",
    "    if first:\n",
    "        next_day = start_day + dt.timedelta(days = 1)\n",
    "        first = False\n",
    "    else:\n",
    "        next_day = next_day + dt.timedelta(days = 1)\n",
    "    \n",
    "    with open(get_path()) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \n",
    "    if str(next_day) not in lines:\n",
    "        print('New day, writing {}'.format(str(next_day)))\n",
    "        \n",
    "        with open(get_path(), 'a') as f:\n",
    "            f.write(str(next_day)+'\\n')\n",
    "    \n",
    "    #print(str(start_day), str(next_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(get_path()) as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "'hello' in lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For all chunks get a pyspark dataframe and dump each in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features_to_pickle(file_counter):\n",
    "    \n",
    "    l = [('CM{}'.format(np.random.randint(100)),np.random.rand()) for i in range(10)]\n",
    "\n",
    "    schema = StructType([StructField('Id', StringType(), True),\n",
    "                     StructField('speed', DoubleType(), True),\n",
    "    ]\n",
    "                     )\n",
    "\n",
    "    rdd = sc.parallelize(l)\n",
    "\n",
    "    df = sqlContext.createDataFrame(rdd,schema)\n",
    "\n",
    "    rows = df.collect()\n",
    "    \n",
    "    a = [row.asDict() for row in row_list]\n",
    "    \n",
    "    script_dir = os.path.dirname(os.getcwd()) #<-- absolute dir the script is in\n",
    "    rel_path = 'trips/trips_{}.pkl'.format(file_counter)\n",
    "    abs_file_path = os.path.join(script_dir, rel_path)\n",
    "    with open(abs_file_path, 'wb') as f:\n",
    "        pickle.dump(a, f)\n",
    "    \n",
    "\n",
    "for i, chunk in enumerate(test): features_to_pickle(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetch all pickled pyspark dataframe files from directory and combine them in one pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CM41</td>\n",
       "      <td>0.61700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CM38</td>\n",
       "      <td>0.02875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.79116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.38107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.13130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CM4</td>\n",
       "      <td>0.70334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CM61</td>\n",
       "      <td>0.76189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.42782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CM34</td>\n",
       "      <td>0.82503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CM33</td>\n",
       "      <td>0.88952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CM41</td>\n",
       "      <td>0.61700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CM38</td>\n",
       "      <td>0.02875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.79116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.38107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.13130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CM4</td>\n",
       "      <td>0.70334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CM61</td>\n",
       "      <td>0.76189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.42782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CM34</td>\n",
       "      <td>0.82503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CM33</td>\n",
       "      <td>0.88952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CM41</td>\n",
       "      <td>0.61700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CM38</td>\n",
       "      <td>0.02875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.79116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.38107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.13130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CM4</td>\n",
       "      <td>0.70334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CM61</td>\n",
       "      <td>0.76189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.42782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CM34</td>\n",
       "      <td>0.82503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CM33</td>\n",
       "      <td>0.88952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>CM41</td>\n",
       "      <td>0.61700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>CM38</td>\n",
       "      <td>0.02875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.79116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.38107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.13130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>CM4</td>\n",
       "      <td>0.70334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>CM61</td>\n",
       "      <td>0.76189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.42782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>CM34</td>\n",
       "      <td>0.82503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>CM33</td>\n",
       "      <td>0.88952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>CM41</td>\n",
       "      <td>0.61700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>CM38</td>\n",
       "      <td>0.02875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.79116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.38107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.13130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>CM4</td>\n",
       "      <td>0.70334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>CM61</td>\n",
       "      <td>0.76189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.42782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>CM34</td>\n",
       "      <td>0.82503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>CM33</td>\n",
       "      <td>0.88952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>CM41</td>\n",
       "      <td>0.61700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>CM38</td>\n",
       "      <td>0.02875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.79116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.38107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>CM46</td>\n",
       "      <td>0.13130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>CM4</td>\n",
       "      <td>0.70334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>CM61</td>\n",
       "      <td>0.76189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>CM79</td>\n",
       "      <td>0.42782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>CM34</td>\n",
       "      <td>0.82503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>CM33</td>\n",
       "      <td>0.88952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   speed\n",
       "0   CM41 0.61700\n",
       "1   CM38 0.02875\n",
       "2   CM79 0.79116\n",
       "3   CM46 0.38107\n",
       "4   CM46 0.13130\n",
       "5    CM4 0.70334\n",
       "6   CM61 0.76189\n",
       "7   CM79 0.42782\n",
       "8   CM34 0.82503\n",
       "9   CM33 0.88952\n",
       "10  CM41 0.61700\n",
       "11  CM38 0.02875\n",
       "12  CM79 0.79116\n",
       "13  CM46 0.38107\n",
       "14  CM46 0.13130\n",
       "15   CM4 0.70334\n",
       "16  CM61 0.76189\n",
       "17  CM79 0.42782\n",
       "18  CM34 0.82503\n",
       "19  CM33 0.88952\n",
       "20  CM41 0.61700\n",
       "21  CM38 0.02875\n",
       "22  CM79 0.79116\n",
       "23  CM46 0.38107\n",
       "24  CM46 0.13130\n",
       "25   CM4 0.70334\n",
       "26  CM61 0.76189\n",
       "27  CM79 0.42782\n",
       "28  CM34 0.82503\n",
       "29  CM33 0.88952\n",
       "..   ...     ...\n",
       "50  CM41 0.61700\n",
       "51  CM38 0.02875\n",
       "52  CM79 0.79116\n",
       "53  CM46 0.38107\n",
       "54  CM46 0.13130\n",
       "55   CM4 0.70334\n",
       "56  CM61 0.76189\n",
       "57  CM79 0.42782\n",
       "58  CM34 0.82503\n",
       "59  CM33 0.88952\n",
       "60  CM41 0.61700\n",
       "61  CM38 0.02875\n",
       "62  CM79 0.79116\n",
       "63  CM46 0.38107\n",
       "64  CM46 0.13130\n",
       "65   CM4 0.70334\n",
       "66  CM61 0.76189\n",
       "67  CM79 0.42782\n",
       "68  CM34 0.82503\n",
       "69  CM33 0.88952\n",
       "70  CM41 0.61700\n",
       "71  CM38 0.02875\n",
       "72  CM79 0.79116\n",
       "73  CM46 0.38107\n",
       "74  CM46 0.13130\n",
       "75   CM4 0.70334\n",
       "76  CM61 0.76189\n",
       "77  CM79 0.42782\n",
       "78  CM34 0.82503\n",
       "79  CM33 0.88952\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_pickles_to_one_df():\n",
    "    \n",
    "    script_dir = os.path.dirname(os.getcwd()) #<-- absolute dir the script is in\n",
    "    \n",
    "    rel_path = \"trips/\"\n",
    "    \n",
    "    abs_folder_path = os.path.join(script_dir, rel_path)\n",
    "    \n",
    "    liste = []\n",
    "    \n",
    "    switch = True\n",
    "    \n",
    "    for file in os.listdir(abs_folder_path):\n",
    "        \n",
    "        abs_file_path = os.path.join(abs_folder_path, file)\n",
    "        \n",
    "\n",
    "        \n",
    "        with open(abs_file_path, 'rb') as f:  \n",
    "    \n",
    "            unpickled_file = pickle.load(f)\n",
    "    \n",
    "            liste.append(pd.DataFrame(unpickled_file))\n",
    "        \n",
    "            while switch:\n",
    "                \n",
    "                print('y')\n",
    "                \n",
    "                df = pd.DataFrame(unpickled_file)\n",
    "                \n",
    "                switch = False\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            dff = pd.DataFrame(unpickled_file)\n",
    "                \n",
    "            df = df.append(dff, ignore_index = True)\n",
    "                \n",
    "        \n",
    "    return df\n",
    "    \n",
    "df = combine_pickles_to_one_df()\n",
    "    \n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-30 20:00:00\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "a = datetime.datetime(2017, 5, 30, 20,0,0)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()\n",
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.port', '49823'),\n",
       " ('spark.app.id', 'local-1505851443817'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '192.168.178.49'),\n",
       " ('spark.app.name', 'My App2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of working on partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_mapped: [('t4', ['t4', 0, 51, 50, 1.1, -0.1, 9.8]), ('t2', ['t2', 10, 48, 50, 0.8, -0.2, 9.8]), ('t2', ['t2', 12, 54, 50, 0.5, -1.0, 9.8]), ('t2', ['t2', 5, 35, 30, 1.1, -0.1, 9.8]), ('t2', ['t2', 6, 38, 30, 0.8, -0.2, 9.8]), ('t2', ['t2', 7, 34, 30, 0.5, -1.0, 9.8])]\n",
      "[{'t4': 0}]\n",
      "StructType(List(StructField(customid,StringType,true),StructField(procid,StringType,true),StructField(speed,DoubleType,true),StructField(timestamp,TimestampType,true)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('CM1', ['CM1', 'aa1', 0.0, datetime.datetime(2017, 5, 30, 20, 0)]),\n",
       " ('CM1', ['CM1', 'aa1', 5.0, datetime.datetime(2017, 5, 30, 20, 1)]),\n",
       " ('CM1', ['CM1', 'aa1', 10.0, datetime.datetime(2017, 5, 30, 20, 2)]),\n",
       " ('CM1', ['CM1', 'aa1', 15.0, datetime.datetime(2017, 5, 30, 20, 2, 30)]),\n",
       " ('CM1', ['CM1', 'bb1', 0.0, datetime.datetime(2017, 5, 30, 19, 0)]),\n",
       " ('CM1', ['CM1', 'bb1', 3.0, datetime.datetime(2017, 5, 30, 19, 1)]),\n",
       " ('CM1', ['CM1', 'bb1', 6.0, datetime.datetime(2017, 5, 30, 19, 2)]),\n",
       " ('CM1', ['CM1', 'bb1', 9.0, datetime.datetime(2017, 5, 30, 19, 9)]),\n",
       " ('CM2', ['CM2', 'cc2', 2.0, datetime.datetime(2017, 5, 30, 23, 0)]),\n",
       " ('CM2', ['CM2', 'cc2', 4.0, datetime.datetime(2017, 5, 30, 23, 1)]),\n",
       " ('CM2', ['CM2', 'cc2', 8.0, datetime.datetime(2017, 5, 30, 23, 2)]),\n",
       " ('CM2', ['CM2', 'cc2', 2.0, datetime.datetime(2017, 5, 30, 23, 3)])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exec(open('script.py').read())\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, HiveContext\n",
    "from operator import itemgetter\n",
    "#conf = SparkConf().setAppName(\"temp\")\n",
    "#conf = SparkConf().setMaster(\"local\").setAppName(\"My App2\").setSystemProperty('spark.executor.memory', '8000mb')\n",
    "#sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "hiveCtx=HiveContext(sc)\n",
    "\n",
    "\n",
    "NUMBER_OF_PARTITIONS = 1\n",
    "\n",
    "\n",
    "def get_feature_1(data):\n",
    "\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    feature = 0\n",
    "    was_excess = False\n",
    "    trip_old = \"no_trip\"\n",
    "    for key_value in data:\n",
    "\n",
    "        if key_value:\n",
    "            key = list(key_value.keys())[0]\n",
    "            value = key_value[key]\n",
    "            if key not in result:\n",
    "                result[key] = 0\n",
    "            value = sorted(value, key=itemgetter(0,1))\n",
    "            #result = (key, value)\n",
    "            #print(value_sorted)\n",
    "            \n",
    "            for v in value:\n",
    "\n",
    "                time = v[1]\n",
    "                trip = v[0]\n",
    "\n",
    "                if trip != trip_old:\n",
    "                    was_excess = False\n",
    "                if was_excess:\n",
    "                    feature = (time - time_old)\n",
    "                    if key in result:\n",
    "                        result[key] += feature\n",
    "                    else:\n",
    "                        result[key] = feature\n",
    "\n",
    "                    was_excess = False\n",
    "\n",
    "                if v[2] - v[3] > 0:\n",
    "                    was_excess = True\n",
    "\n",
    "                time_old = time\n",
    "                trip_old = trip\n",
    "            \n",
    "    return result\n",
    "    \n",
    "def get_feature_2(data):\n",
    "\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    feature = 0\n",
    "    was_excess = False\n",
    "    trip_old = \"no_trip\"\n",
    "    for key_value in data:\n",
    "\n",
    "        if key_value:\n",
    "            key = list(key_value.keys())[0]\n",
    "            value = key_value[key]\n",
    "            if key not in result:\n",
    "                result[key] = 0\n",
    "            value = sorted(value, key=itemgetter(0,1))\n",
    "            #result = (key, value)\n",
    "            #print(value_sorted)\n",
    "\n",
    "            for v in value:\n",
    "\n",
    "                time = v[1]\n",
    "                trip = v[0]\n",
    "                if key in result:\n",
    "                    result[key] += feature\n",
    "                else:\n",
    "                    result[key] = feature\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def generate_trip_features_on_partition(iterator):\n",
    "    data = {}\n",
    "    data2 = []\n",
    "    for key, value in iterator:\n",
    "        data2.append((key, value))\n",
    "        if key in data:\n",
    "            data[key] +=  [value]\n",
    "            t = 1\n",
    "        else:\n",
    "            data[key] = [value]\n",
    "  \n",
    "    print('data:',data)\n",
    "    feature_1 = get_feature_1([data])\n",
    "    feature_2 = get_feature_2([data])\n",
    "\n",
    "\n",
    "    trips = set(list(feature_1.keys()) + list(feature_2.keys()))\n",
    "    print('trips', trips)\n",
    "    result = {}    \n",
    "    for trip in trips:\n",
    "        result[trip] = {'f1':0, 'f2':0}\n",
    "        if trip in feature_1:\n",
    "            result[trip]['f1'] = feature_1[trip]\n",
    "        if trip in feature_2:\n",
    "            result[trip]['f2'] = feature_2[trip]\n",
    "\n",
    "    return [feature_2]\n",
    "    #return [result]\n",
    "\n",
    "def generate_trip_features(data):\n",
    "\n",
    "            NUMBER_OF_PARTITIONS = 1\n",
    "            #data_rdd = data.rdd\n",
    "            data_rdd = sc.parallelize([ ['t4', 0, 51, 50, 1.1, -0.1, 9.8],\\\n",
    "                                       ['t2', 10, 48, 50, 0.8, -0.2, 9.8],\\\n",
    "                                       ['t2', 12, 54, 50, 0.5, -1.0 , 9.8],\\\n",
    "                                       ['t2', 5, 35, 30, 1.1, -0.1,9.8],\\\n",
    "                                       ['t2', 6, 38, 30, 0.8, -0.2, 9.8],\\\n",
    "                                       ['t2', 7, 34, 30, 0.5, -1.0 , 9.8] ])\n",
    "\n",
    "            data_mapped = data_rdd.map(lambda x: (x[0], [a for a in x])).partitionBy(NUMBER_OF_PARTITIONS)\n",
    "            print('data_mapped:', data_mapped.collect())\n",
    "            results = data_mapped.mapPartitions(generate_trip_features_on_partition).collect()\n",
    "        \n",
    "            print(results)        \n",
    "\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input generte trip features\n",
      "\n",
      "[('CM1', ['CM1', 'aa1', 0.0, 1]),\n",
      " ('CM1', ['CM1', 'aa1', 5.0, 2]),\n",
      " ('CM1', ['CM1', 'aa1', 10.0, 3]),\n",
      " ('CM1', ['CM1', 'aa1', 15.0, 3.544]),\n",
      " ('CM1', ['CM1', 'bb1', 0.0, 1.5]),\n",
      " ('CM1', ['CM1', 'bb1', 3.0, 1.2]),\n",
      " ('CM1', ['CM1', 'bb1', 6.0, 1.011]),\n",
      " ('CM1', ['CM1', 'bb1', 9.0, 3.5]),\n",
      " ('CM2', ['CM2', 'cc2', 2.0, 31.5]),\n",
      " ('CM2', ['CM2', 'cc2', 4.0, 4.5]),\n",
      " ('CM2', ['CM2', 'cc2', 8.0, 32.5]),\n",
      " ('CM2', ['CM2', 'cc2', 2.0, 399.5])]\n",
      "input get_speed_features\n",
      "\n",
      "{'CM1': [['CM1', 'aa1', 0.0, 1],\n",
      "         ['CM1', 'aa1', 5.0, 2],\n",
      "         ['CM1', 'aa1', 10.0, 3],\n",
      "         ['CM1', 'aa1', 15.0, 3.544],\n",
      "         ['CM1', 'bb1', 0.0, 1.5],\n",
      "         ['CM1', 'bb1', 3.0, 1.2],\n",
      "         ['CM1', 'bb1', 6.0, 1.011],\n",
      "         ['CM1', 'bb1', 9.0, 3.5]],\n",
      " 'CM2': [['CM2', 'cc2', 2.0, 31.5],\n",
      "         ['CM2', 'cc2', 4.0, 4.5],\n",
      "         ['CM2', 'cc2', 8.0, 32.5],\n",
      "         ['CM2', 'cc2', 2.0, 399.5]]}\n",
      "after sort\n",
      "\n",
      "[['CM2', 'cc2', 4.0, 4.5],\n",
      " ['CM2', 'cc2', 2.0, 31.5],\n",
      " ['CM2', 'cc2', 8.0, 32.5],\n",
      " ['CM2', 'cc2', 2.0, 399.5]]\n",
      "tpr_offense:  4.5\n",
      "tpr_offense:  1.0\n",
      "after sort\n",
      "\n",
      "[['CM1', 'aa1', 0.0, 1],\n",
      " ['CM1', 'bb1', 6.0, 1.011],\n",
      " ['CM1', 'bb1', 3.0, 1.2],\n",
      " ['CM1', 'bb1', 0.0, 1.5],\n",
      " ['CM1', 'aa1', 5.0, 2],\n",
      " ['CM1', 'aa1', 10.0, 3],\n",
      " ['CM1', 'bb1', 9.0, 3.5],\n",
      " ['CM1', 'aa1', 15.0, 3.544]]\n",
      "tpr_offense:  0.010999999999999899\n",
      "tpr_offense:  0.5\n",
      "tpr_offense:  1\n",
      "tpr_offense:  0.5\n",
      "tpr_offense:  0.04400000000000004\n",
      "Output get_speed_feature\n",
      "\n",
      "{'CM1': ('ft1', 2.055), 'CM2': ('ft1', 5.5)}\n",
      "input get_speed_features\n",
      "\n",
      "{'CM1': [['CM1', 'aa1', 0.0, 1],\n",
      "         ['CM1', 'aa1', 5.0, 2],\n",
      "         ['CM1', 'aa1', 10.0, 3],\n",
      "         ['CM1', 'aa1', 15.0, 3.544],\n",
      "         ['CM1', 'bb1', 0.0, 1.5],\n",
      "         ['CM1', 'bb1', 3.0, 1.2],\n",
      "         ['CM1', 'bb1', 6.0, 1.011],\n",
      "         ['CM1', 'bb1', 9.0, 3.5]],\n",
      " 'CM2': [['CM2', 'cc2', 2.0, 31.5],\n",
      "         ['CM2', 'cc2', 4.0, 4.5],\n",
      "         ['CM2', 'cc2', 8.0, 32.5],\n",
      "         ['CM2', 'cc2', 2.0, 399.5]]}\n",
      "after sort\n",
      "\n",
      "[['CM2', 'cc2', 4.0, 4.5],\n",
      " ['CM2', 'cc2', 2.0, 31.5],\n",
      " ['CM2', 'cc2', 8.0, 32.5],\n",
      " ['CM2', 'cc2', 2.0, 399.5]]\n",
      "tpr_offense:  1.0\n",
      "after sort\n",
      "\n",
      "[['CM1', 'aa1', 0.0, 1],\n",
      " ['CM1', 'bb1', 6.0, 1.011],\n",
      " ['CM1', 'bb1', 3.0, 1.2],\n",
      " ['CM1', 'bb1', 0.0, 1.5],\n",
      " ['CM1', 'aa1', 5.0, 2],\n",
      " ['CM1', 'aa1', 10.0, 3],\n",
      " ['CM1', 'bb1', 9.0, 3.5],\n",
      " ['CM1', 'aa1', 15.0, 3.544]]\n",
      "tpr_offense:  0.010999999999999899\n",
      "tpr_offense:  1\n",
      "tpr_offense:  0.5\n",
      "tpr_offense:  0.04400000000000004\n",
      "Output get_speed_feature\n",
      "\n",
      "{'CM1': ('ft2', 1.555), 'CM2': ('ft2', 1.0)}\n",
      "delete asap [{'CM2': ('ft1', 5.5), 'CM1': ('ft1', 2.055)}, {'CM2': ('ft2', 1.0), 'CM1': ('ft2', 1.555)}] {'CM2', 'CM1'}\n"
     ]
    }
   ],
   "source": [
    "#http://parrotprediction.com/partitioning-in-apache-spark/\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "\n",
    "def get_dataframe():\n",
    "    \n",
    "        l = [('CM1','aa1', 00.0,datetime.datetime(2017, 5, 30, 20,0,0)),\\\n",
    "             ('CM1','aa1', 5.0,datetime.datetime(2017, 5, 30, 20,1,0)),\\\n",
    "             ('CM1','aa1', 10.0,datetime.datetime(2017, 5, 30, 20,2,0)),\\\n",
    "            ('CM1','aa1', 15.0,datetime.datetime(2017, 5, 30, 20,2,30)),\\\n",
    "            ('CM1','bb1', 00.0,datetime.datetime(2017, 5, 30, 19,0,0)),\\\n",
    "            ('CM1','bb1', 3.0,datetime.datetime(2017, 5, 30, 19,1,0)),\\\n",
    "            ('CM1','bb1', 6.0,datetime.datetime(2017, 5, 30, 19,2,0)),\\\n",
    "            ('CM1','bb1', 9.0,datetime.datetime(2017, 5, 30, 19,9,0)),\\\n",
    "            ('CM2','cc2', 2.0,datetime.datetime(2017, 5, 30, 23,0,0)),\\\n",
    "            ('CM2','cc2', 4.0,datetime.datetime(2017, 5, 30, 23,1,0)),\\\n",
    "            ('CM2','cc2', 8.0,datetime.datetime(2017, 5, 30, 23,2,0)),\\\n",
    "            ('CM2','cc2', 2.0,datetime.datetime(2017, 5, 30, 23,3,0)),\\\n",
    "            ]\n",
    "\n",
    "        schema = StructType([StructField('customid', StringType(), True),\n",
    "                     StructField('procid', StringType(), True),\n",
    "                     StructField('speed', DoubleType(), True),\n",
    "                     StructField('timestamp', TimestampType(), True)]\n",
    "                     )\n",
    "\n",
    "        rdd = sc.parallelize(l)\n",
    "\n",
    "        df = sqlContext.createDataFrame(rdd,schema)\n",
    "\n",
    "        #print(df.schema)\n",
    "        \n",
    "        df.show()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "def get_speed_feature(trips_as_dict, limit = 3):\n",
    "    \"\"\"\n",
    "    :input:   type dixt\n",
    "            {'CM1': [['CM1', 'aa1', 0.0, 1],\n",
    "                    ['CM1', 'aa1', 5.0, 2],\n",
    "                     ['CM1', 'aa1', 10.0, 3],\n",
    "                     ['CM1', 'aa1', 15.0, 3.5],\n",
    "                     ['CM1', 'bb1', 0.0, 3.5],\n",
    "                     ['CM1', 'bb1', 3.0, 3.5],\n",
    "                     ['CM1', 'bb1', 6.0, 3.5],\n",
    "                     ['CM1', 'bb1', 9.0, 3.5]],\n",
    "             'CM2': [['CM2', 'cc2', 2.0, 3.5],\n",
    "                     ['CM2', 'cc2', 4.0, 3.5],\n",
    "                     ['CM2', 'cc2', 8.0, 3.5],\n",
    "                     ['CM2', 'cc2', 2.0, 3.5]]}\n",
    "                     \n",
    "    :output: {'CM1':{ft1:99, ft2:27272, ...}, 'CM2':{ft1:888, ft2:27272,...}}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    print('input get_speed_features\\n')\n",
    "    pprint(trips_as_dict)\n",
    "    \n",
    "    overall_result = {}\n",
    "    result = {}\n",
    "    for (trip_id, data_point_list) in trips_as_dict.items():\n",
    "        \n",
    "        #print(trip_id, data_point_list) #(s)\n",
    "        \n",
    "        #print('before sort\\n')\n",
    "        #pprint(data_point_list) # (s)\n",
    "        data_point_list = sorted(data_point_list, key = itemgetter(0,3))       \n",
    "        print('after sort\\n')\n",
    "        pprint(data_point_list) # (s)\n",
    "        \n",
    "        counter = 0\n",
    "        for number, data_point in enumerate(data_point_list):\n",
    "            if data_point[2] > limit:\n",
    "                if number == 0:\n",
    "                    timeperiod_of_offense = data_point_list[number][3]\n",
    "                else:\n",
    "                    timeperiod_of_offense = data_point_list[number][3] -\\\n",
    "                                            data_point_list[number-1][3]\n",
    "                print('tpr_offense: ', timeperiod_of_offense)\n",
    "                counter +=1*timeperiod_of_offense\n",
    "        result[trip_id] = ('ft1',counter)\n",
    "    \n",
    "    print('Output get_speed_feature\\n')\n",
    "    pprint(result)\n",
    "    \n",
    "    return result\n",
    "            \n",
    "def get_speed_feature2(trips_as_dict, limit = 5):\n",
    "    \"\"\"\n",
    "    :input:   type dixt\n",
    "            {'CM1': [['CM1', 'aa1', 0.0, 1],\n",
    "                    ['CM1', 'aa1', 5.0, 2],\n",
    "                     ['CM1', 'aa1', 10.0, 3],\n",
    "                     ['CM1', 'aa1', 15.0, 3.5],\n",
    "                     ['CM1', 'bb1', 0.0, 3.5],\n",
    "                     ['CM1', 'bb1', 3.0, 3.5],\n",
    "                     ['CM1', 'bb1', 6.0, 3.5],\n",
    "                     ['CM1', 'bb1', 9.0, 3.5]],\n",
    "             'CM2': [['CM2', 'cc2', 2.0, 3.5],\n",
    "                     ['CM2', 'cc2', 4.0, 3.5],\n",
    "                     ['CM2', 'cc2', 8.0, 3.5],\n",
    "                     ['CM2', 'cc2', 2.0, 3.5]]}\n",
    "                     \n",
    "    :output: {'CM1':{ft1:99, ft2:27272, ...}, 'CM2':{ft1:888, ft2:27272,...}}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    print('input get_speed_features\\n')\n",
    "    pprint(trips_as_dict)\n",
    "    \n",
    "    overall_result = {}\n",
    "    result = {}\n",
    "    for (trip_id, data_point_list) in trips_as_dict.items():\n",
    "        \n",
    "        #print(trip_id, data_point_list) #(s)\n",
    "        \n",
    "        #print('before sort\\n')\n",
    "        #pprint(data_point_list) # (s)\n",
    "        data_point_list = sorted(data_point_list, key = itemgetter(0,3))       \n",
    "        print('after sort\\n')\n",
    "        pprint(data_point_list) # (s)\n",
    "        \n",
    "        counter = 0\n",
    "        for number, data_point in enumerate(data_point_list):\n",
    "            if data_point[2] > limit:\n",
    "                if number == 0:\n",
    "                    timeperiod_of_offense = data_point_list[number][3]\n",
    "                else:\n",
    "                    timeperiod_of_offense = data_point_list[number][3] -\\\n",
    "                                            data_point_list[number-1][3]\n",
    "                print('tpr_offense: ', timeperiod_of_offense)\n",
    "                counter +=1*timeperiod_of_offense\n",
    "        result[trip_id] = ('ft2',counter)\n",
    "    \n",
    "    print('Output get_speed_feature\\n')\n",
    "    pprint(result)\n",
    "    \n",
    "    return result\n",
    "            \n",
    "            \n",
    "def generate_trip_features_on_partition(kv_iterator):\n",
    "    \"\"\"\n",
    "    Hint: using k-v pairs where they key is inside the value again.\\\n",
    "          Function must get and retur an iterable.\n",
    "          Immutable are ints, strings and tuples.\n",
    "    \n",
    "    Input: KV-tuple aus trip-id und Fahrdaten\n",
    "    ('CM1', ['CM1', 'bb1', 9.0, 3.5])\n",
    "    ('CM1', ['CM1', 'bb1', 9.0, 9.5])\n",
    "    ('CM1', ['CM1', 'bb1', 9.0, 9.5])\n",
    "    ('CM2', ['CM2', 'cc2', 2.0, 3.5]),\n",
    "    \n",
    "    Output: list(dict('CM1':{tripID: 'CM1', feature1: value, .....},\n",
    "                      'CM2':{tripID: 'CM2', feature1: value, .....}))\n",
    "    \n",
    "    \"\"\"\n",
    "    print('input generte trip features\\n')\n",
    "    pprint(kv_iterator)\n",
    "    \n",
    "    S = {}\n",
    "    key_list = set()\n",
    "    for key, value in kv_iterator:\n",
    "        \n",
    "        key_list = key_list | set([key]) # set works on iterables -> just the key-string would get split\n",
    "        \n",
    "        if key not in S.keys():\n",
    "            S[key] = [value]\n",
    "        \n",
    "        else:\n",
    "            S[key] += [value]\n",
    "    all_features_per_trip = []\n",
    "    all_features_per_trip.append(get_speed_feature(S)) # {'CM1': ('ft1', 2.055), 'CM2': ('ft1', 5.5)})\n",
    "    all_features_per_trip.append(get_speed_feature2(S)) # {'CM1': ('ft1', 2.055), 'CM2': ('ft1', 5.5)})\n",
    "    \n",
    "\n",
    "    result = {key:{feature_list in all_features_per_trip} for key in key_list}\n",
    "\n",
    "\"\"\"\n",
    "df =  get_dataframe()\n",
    "\n",
    "df_mapped = df.rdd.map(lambda x: (x[0], [a for a in x]))\n",
    "pprint(df_mapped.collect()) #(s)\n",
    "\n",
    "#generate_trip_features(0)\n",
    "df_mapped = df_mapped.collect()# (t)\n",
    "\"\"\"\n",
    "df_mapped = [('CM1', ['CM1', 'aa1', 0.0, 1]),\n",
    " ('CM1', ['CM1', 'aa1', 5.0, 2]),\n",
    " ('CM1', ['CM1', 'aa1', 10.0, 3]),\n",
    " ('CM1', ['CM1', 'aa1', 15.0, 3.544]),\n",
    " ('CM1', ['CM1', 'bb1', 0.0, 1.5]),\n",
    " ('CM1', ['CM1', 'bb1', 3.0, 1.2]),\n",
    " ('CM1', ['CM1', 'bb1', 6.0, 1.011]),\n",
    " ('CM1', ['CM1', 'bb1', 9.0, 3.5]),\n",
    " ('CM2', ['CM2', 'cc2', 2.0, 31.5]),\n",
    " ('CM2', ['CM2', 'cc2', 4.0, 4.5]),\n",
    " ('CM2', ['CM2', 'cc2', 8.0, 32.5]),\n",
    " ('CM2', ['CM2', 'cc2', 2.0, 399.5])]\n",
    "generate_trip_features_on_partition(df_mapped) # (t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['b', 'a', 'c']) dict_values([2, 1, 3]) dict_items([('b', 2), ('a', 1), ('c', 3)])\n",
      "b\n",
      "a\n",
      "c\n",
      "[1, 2, 3, 4, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 1]\n",
      "{'f': 10}\n"
     ]
    }
   ],
   "source": [
    "S = {'a':1, 'b':2, 'c':3}\n",
    "O = {'a':{'f':10}, 'b':{'h':20}, 'c':{'g':190}}\n",
    "L = [1,2,3,4]\n",
    "\n",
    "print(S.keys(), S.values(), S.items())\n",
    "for i in S: print(i)\n",
    "\n",
    "print(L+L)\n",
    "L.append(1)\n",
    "print (L)\n",
    "\n",
    "print(O['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
